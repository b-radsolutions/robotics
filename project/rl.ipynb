{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df7279-b580-461d-87eb-99789cec93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545d0884-faf8-40f2-8889-026cee5f852e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f22ad3e76a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9fd3b-8c6c-40b2-a2d1-b919368f96d3",
   "metadata": {},
   "source": [
    "Defines Arm related functions for moving the arm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d5f8e-21f2-4536-97e5-3651c0072333",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arm = Arm_Device() # Get DOFBOT object\n",
    "def set_angles(arm, angles):\n",
    "    for joint, angle in enumerate(angles):\n",
    "        moveJoint(arm, joint+1, angle, 1000)\n",
    "        \n",
    "def readAllActualJointAngles():\n",
    "    q = np.array([arm.Arm_serial_servo_read(1),arm.Arm_serial_servo_read(2),arm.Arm_serial_servo_read(3),arm.Arm_serial_servo_read(4),arm.Arm_serial_servo_read(5),arm.Arm_serial_servo_read(6)])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba3c97-ac8e-4d98-8ddf-958933a56a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time #import the time module. Used for adding pauses during operation\n",
    "from Arm_Lib import Arm_Device #import the module associated with the arm\n",
    "\n",
    "from general_robotics_toolbox import fwdkin, Robot\n",
    "import general_robotics_toolbox as rox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a7989-bfa5-4d5a-83e8-eecb56acdb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for gpu\n",
    "device = torch.device(\"cuda\" if torch.cude.is_available() else \"cpu\")\n",
    "print(f'Using {device} (device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438eb14-254f-40b9-8c34-e270ae5004bd",
   "metadata": {},
   "source": [
    "Define the architechure for the Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b64d3-fc62-49cc-8030-2994efb8cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural networks\n",
    "\"\"\"\n",
    "The critic network learns an approximation of the value function\n",
    "with a neural network\n",
    "\"\"\"\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\"\"\"\n",
    "The actor network learns an approximation of the policy function\n",
    "with a neural network\n",
    "\"\"\"\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "actor_net = ActorNetwork().to(device)\n",
    "critic_net = CrticiNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef10ca-6f99-484d-8685-6ee683c6dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes: int, arm: Arm):\n",
    "    for ep in episodes:\n",
    "        # initialize the arm to a random state. \n",
    "        set_angles(arm, np.random.rand(5, 1)*180)\n",
    "        # select target destination for arm\n",
    "        \"\"\"\n",
    "        target_end_effector is used to evaluate the model.\n",
    "        The farther the euclidean distance from the end effector,\n",
    "        the lower the reward.\n",
    "        \"\"\"\n",
    "        target_angles = np.random.rand(5, 1)*180)\n",
    "        target_end_effector_pose = fwdkin(dofbot, target_angles)\n",
    "\n",
    "        \"\"\"\n",
    "        the state are the current servo\n",
    "        \"\"\"\n",
    "        state = readAllActualJointAngles(Arm)\n",
    "        \n",
    "        \"\"\"\n",
    "        actions is an array of tuples for each joint)\n",
    "        [[p1, t1], [p2, t2], [p3, v3], [p4, v4], [p5, v5], [p5, v5]]\n",
    "        \"\"\"\n",
    "        actions = np.reshape((actor_net(state).numpy(), (5, 2))\n",
    "        for num, (position, useconds) in enumerate(actions, 1):\n",
    "            arm.Arm_serial_servo_write(num, position, useconds)          \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce038f-b256-4018-a0d1-cce9aa423e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8eafe6-a26f-47b1-829c-3e672cdf2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize actor network with softmax output for action probabilities\n",
    "Initialize critic network for state value estimation\n",
    "\n",
    "for each episode:\n",
    "    Initialize state\n",
    "    while not done:\n",
    "        action_probabilities = actor_network(state)\n",
    "        action = choose_action_based_on_probabilities(action_probabilities)\n",
    "        next_state, reward, done = environment.step(action)\n",
    "        \n",
    "        # Critic update\n",
    "        value = critic_network(state)\n",
    "        next_value = critic_network(next_state)\n",
    "        td_target = reward + (gamma * next_value * (1 - done))\n",
    "        td_error = td_target - value\n",
    "        critic_loss = td_error ** 2\n",
    "        update critic_network to minimize critic_loss\n",
    "\n",
    "        # Actor update\n",
    "        advantage = td_error\n",
    "        actor_loss = -log(action_probabilities[action]) * advantage\n",
    "        update actor_network to minimize actor_loss\n",
    "\n",
    "        state = next_state\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
